1. improve the training tradegy
originally pretrain+finetune


（要改数据集和训练模式（主要是模型输出的最后一层））
pretrain: 原先的做法是用分类来做的，我们可以用生成式的方式做pretraining
第一种是用GPT的模式(文本这块：MLM是Bert，我们用GPT的自回归的方式去做训练)
图文对齐：目标是做图文对齐的预训练：查Openai和Meta是怎么做图文对齐的，有经典方法：CLIP和BLIP(可搜到，开源)

Consider using finetuning（既有分类的，又有生成的）
可以把很多不同类型的任务归一类任务（可以把分类和生成全都转换为生成式的）
这样所有任务就可以同时训练了/
好处：
1/最后只需要得到一类模型，就可以解决里面所有的任务。
2/不同数据集里面的任务比较接近的时候，在训练的时候是有相互促进的影响的。

VILT的两个Loss函数第一个是MLM第二个是图文对齐。
MLM主要是学习分类模式的语义的，我们要替换模型结构和loss函数去学习生成式的语义（GPT）
图文对齐中，VILT提供了一种方式，我们可以去查找OpenAI和Meta的方式。


任务先后顺序：

Pretrain：

//数据：
//找到数据，用来做预训练的数据是什么

看多模态预训练的数据格式是什么（符合程序段就可以了）（看图文对齐预训练）

程序段：
把MLM这部分变成GPT模式（不用Bert用GPT，换模型）

Finetune:

把分类和生成全都转换为生成式的，改模型最后一层。
原先有一些分类任务，转成生成任务。
分类最后一层的维度是类数，但是最后一层的维度是词库大小。


还可能改的是：训练策略
去看最近图文大模型的训练策略。
去完成复杂的任务
eg: transformer解决单个任务到【其他结构】解决复合型任务。